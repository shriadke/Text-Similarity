{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample1 = The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you haveany participating brands on your receipt, you'll get points based on the cost of the products. You don't need toclip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find thesavings for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = \"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample2 = The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have anyeligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cutout any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savingsfor you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = \"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample3 = We are always looking for opportunities for you to earn more points, which is why we also give you a selectionof Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points youearn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the pointswhether or not you knew about the offer. We just think it is easier that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample3 = \"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization in sents and words in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sents(text, stop_char=\".\"):\n",
    "    return [x.lower()+  ' ' + stop_char + ''  for x in text.split(stop_char) if x!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words_from_sent(sents, stop_words=[], punct=[]):\n",
    "    return [x.lower() for sent in sents for x in sent.split()  if (x != \"\" and x not in punct and x not in stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['the', 'a', 'an', 'is', 'are', 'will', 'has', 'have', 'had', 'and', 'or', 'we', 'you', 'to', 'with', 'on', 'your', 'for', 'of', 'this', 'that', 'those', 'these', 'because', 'it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['.',':',',','!',';','\\'','\\\"','(',')']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the easiest way to earn points with fetch rewards is to just shop for the products you already love .', ' if you have any participating brands on your receipt, you will get points based on the cost of the products .', ' you do not need to clip any coupons or scan individual barcodes .', ' just scan each grocery receipt after you shop and we will find the savings for you .']\n"
     ]
    }
   ],
   "source": [
    "sent1 = tokenize_sents(decontracted(sample1))\n",
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'easiest', 'way', 'to', 'earn', 'points', 'with', 'fetch', 'rewards', 'is', 'to', 'just', 'shop', 'for', 'the', 'products', 'you', 'already', 'love', '.', 'if', 'you', 'have', 'any', 'participating', 'brands', 'on', 'your', 'receipt,', 'you', 'will', 'get', 'points', 'based', 'on', 'the', 'cost', 'of', 'the', 'products', '.', 'you', 'do', 'not', 'need', 'to', 'clip', 'any', 'coupons', 'or', 'scan', 'individual', 'barcodes', '.', 'just', 'scan', 'each', 'grocery', 'receipt', 'after', 'you', 'shop', 'and', 'we', 'will', 'find', 'the', 'savings', 'for', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "words1 = tokenize_words_from_sent(sent1)\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the easiest way to earn points with fetch rewards is to just shop for the items you already buy .', ' if you have any eligible brands on your receipt, you will get points based on the total cost of the products .', ' you do not need to cut out any coupons or scan individual upcs .', ' just scan your receipt after you check out and we will find the savings for you .']\n"
     ]
    }
   ],
   "source": [
    "sent2 = tokenize_sents(decontracted(sample2))\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'easiest', 'way', 'to', 'earn', 'points', 'with', 'fetch', 'rewards', 'is', 'to', 'just', 'shop', 'for', 'the', 'items', 'you', 'already', 'buy', '.', 'if', 'you', 'have', 'any', 'eligible', 'brands', 'on', 'your', 'receipt,', 'you', 'will', 'get', 'points', 'based', 'on', 'the', 'total', 'cost', 'of', 'the', 'products', '.', 'you', 'do', 'not', 'need', 'to', 'cut', 'out', 'any', 'coupons', 'or', 'scan', 'individual', 'upcs', '.', 'just', 'scan', 'your', 'receipt', 'after', 'you', 'check', 'out', 'and', 'we', 'will', 'find', 'the', 'savings', 'for', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "words2 = tokenize_words_from_sent(sent2)\n",
    "print(words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers', ' These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand', \" No need to pre-select these offers, we'll give you the points whether or not you knew about the offer\", ' We just think it is easier that way', '']\n"
     ]
    }
   ],
   "source": [
    "sent3 = sample3.split(\".\")\n",
    "print(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we are always looking for opportunities for you to earn more points, which is why we also give you a selection of special offers .', ' these special offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand .', ' no need to pre-select these offers, we will give you the points whether or not you knew about the offer .', ' we just think it is easier that way .']\n"
     ]
    }
   ],
   "source": [
    "sent3 = tokenize_sents(decontracted(sample3))\n",
    "print(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'always', 'looking', 'for', 'opportunities', 'for', 'you', 'to', 'earn', 'more', 'points,', 'which', 'is', 'why', 'we', 'also', 'give', 'you', 'a', 'selection', 'of', 'special', 'offers', '.', 'these', 'special', 'offers', 'are', 'opportunities', 'to', 'earn', 'bonus', 'points', 'on', 'top', 'of', 'the', 'regular', 'points', 'you', 'earn', 'every', 'time', 'you', 'purchase', 'a', 'participating', 'brand', '.', 'no', 'need', 'to', 'pre-select', 'these', 'offers,', 'we', 'will', 'give', 'you', 'the', 'points', 'whether', 'or', 'not', 'you', 'knew', 'about', 'the', 'offer', '.', 'we', 'just', 'think', 'it', 'is', 'easier', 'that', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "words3 = tokenize_words_from_sent(sent3)\n",
    "print(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [sample1,sample2,sample3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 == sample2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Vocabulary and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_words = words1+words2\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'you': 12, 'the': 10, '.': 8, 'to': 6, 'points': 4, 'just': 4, 'for': 4, 'any': 4, 'on': 4, 'will': 4, 'scan': 4, 'shop': 3, 'products': 3, 'your': 3, 'easiest': 2, 'way': 2, 'earn': 2, 'with': 2, 'fetch': 2, 'rewards': 2, 'is': 2, 'already': 2, 'if': 2, 'have': 2, 'brands': 2, 'receipt,': 2, 'get': 2, 'based': 2, 'cost': 2, 'of': 2, 'do': 2, 'not': 2, 'need': 2, 'coupons': 2, 'or': 2, 'individual': 2, 'receipt': 2, 'after': 2, 'and': 2, 'we': 2, 'find': 2, 'savings': 2, 'out': 2, 'love': 1, 'participating': 1, 'clip': 1, 'barcodes': 1, 'each': 1, 'grocery': 1, 'items': 1, 'buy': 1, 'eligible': 1, 'total': 1, 'cut': 1, 'upcs': 1, 'check': 1})\n",
      "*********************\n",
      "['you', 'the', '.', 'to', 'points', 'just', 'for', 'any', 'on', 'will', 'scan', 'shop', 'products', 'your', 'easiest', 'way', 'earn', 'with', 'fetch', 'rewards', 'is', 'already', 'if', 'have', 'brands', 'receipt,', 'get', 'based', 'cost', 'of', 'do', 'not', 'need', 'coupons', 'or', 'individual', 'receipt', 'after', 'and', 'we', 'find', 'savings', 'out', 'love', 'participating', 'clip', 'barcodes', 'each', 'grocery', 'items', 'buy', 'eligible', 'total', 'cut', 'upcs', 'check']\n",
      "*********************\n",
      "{0: 'you', 1: 'the', 2: '.', 3: 'to', 4: 'points', 5: 'just', 6: 'for', 7: 'any', 8: 'on', 9: 'will', 10: 'scan', 11: 'shop', 12: 'products', 13: 'your', 14: 'easiest', 15: 'way', 16: 'earn', 17: 'with', 18: 'fetch', 19: 'rewards', 20: 'is', 21: 'already', 22: 'if', 23: 'have', 24: 'brands', 25: 'receipt,', 26: 'get', 27: 'based', 28: 'cost', 29: 'of', 30: 'do', 31: 'not', 32: 'need', 33: 'coupons', 34: 'or', 35: 'individual', 36: 'receipt', 37: 'after', 38: 'and', 39: 'we', 40: 'find', 41: 'savings', 42: 'out', 43: 'love', 44: 'participating', 45: 'clip', 46: 'barcodes', 47: 'each', 48: 'grocery', 49: 'items', 50: 'buy', 51: 'eligible', 52: 'total', 53: 'cut', 54: 'upcs', 55: 'check'}\n",
      "*********************\n",
      "{'you': 0, 'the': 1, '.': 2, 'to': 3, 'points': 4, 'just': 5, 'for': 6, 'any': 7, 'on': 8, 'will': 9, 'scan': 10, 'shop': 11, 'products': 12, 'your': 13, 'easiest': 14, 'way': 15, 'earn': 16, 'with': 17, 'fetch': 18, 'rewards': 19, 'is': 20, 'already': 21, 'if': 22, 'have': 23, 'brands': 24, 'receipt,': 25, 'get': 26, 'based': 27, 'cost': 28, 'of': 29, 'do': 30, 'not': 31, 'need': 32, 'coupons': 33, 'or': 34, 'individual': 35, 'receipt': 36, 'after': 37, 'and': 38, 'we': 39, 'find': 40, 'savings': 41, 'out': 42, 'love': 43, 'participating': 44, 'clip': 45, 'barcodes': 46, 'each': 47, 'grocery': 48, 'items': 49, 'buy': 50, 'eligible': 51, 'total': 52, 'cut': 53, 'upcs': 54, 'check': 55}\n",
      "*********************\n"
     ]
    }
   ],
   "source": [
    "print(word_counts)\n",
    "print(\"*********************\")\n",
    "print(sorted_vocab)\n",
    "print(\"*********************\")\n",
    "print(int_to_vocab)\n",
    "print(\"*********************\")\n",
    "print(vocab_to_int)\n",
    "print(\"*********************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Word Vectors using TF-IDF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_1 = np.zeros(len(sorted_vocab),dtype=float)\n",
    "word_tf_2 = np.zeros(len(sorted_vocab),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08450704 0.07042254 0.05633803 0.04225352 0.02816901 0.02816901\n",
      " 0.02816901 0.02816901 0.02816901 0.02816901 0.02816901 0.02816901\n",
      " 0.02816901 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451\n",
      " 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451\n",
      " 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451\n",
      " 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451\n",
      " 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451 0.01408451\n",
      " 0.         0.01408451 0.01408451 0.01408451 0.01408451 0.01408451\n",
      " 0.01408451 0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "for word,count in Counter(words1).items():\n",
    "    word_tf_1[vocab_to_int[word]] = count/len(words1)\n",
    "print(word_tf_1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08219178 0.06849315 0.05479452 0.04109589 0.02739726 0.02739726\n",
      " 0.02739726 0.02739726 0.02739726 0.02739726 0.02739726 0.01369863\n",
      " 0.01369863 0.02739726 0.01369863 0.01369863 0.01369863 0.01369863\n",
      " 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863\n",
      " 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863\n",
      " 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863\n",
      " 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863 0.01369863\n",
      " 0.02739726 0.         0.         0.         0.         0.\n",
      " 0.         0.01369863 0.01369863 0.01369863 0.01369863 0.01369863\n",
      " 0.01369863 0.01369863]\n"
     ]
    }
   ],
   "source": [
    "for word,count in Counter(words2).items():\n",
    "    word_tf_2[vocab_to_int[word]] = count/len(words2)\n",
    "print(word_tf_2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf = np.zeros(len(sorted_vocab),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "for word in sorted_vocab:\n",
    "    n_docs = 0\n",
    "    for doc in [words1, words2]:\n",
    "        if word in doc:\n",
    "            n_docs += 1\n",
    "    word_idf[vocab_to_int[word]] = N/n_docs\n",
    "print(word_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_1 = [tf*idf for tf, idf in zip(word_tf_1, word_idf)]\n",
    "tfidf_2 = [tf*idf for tf, idf in zip(word_tf_2, word_idf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08450704225352113, 0.07042253521126761, 0.056338028169014086, 0.04225352112676056, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.014084507042253521, 0.0, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.028169014084507043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] [0.0821917808219178, 0.0684931506849315, 0.0547945205479452, 0.0410958904109589, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0136986301369863, 0.0136986301369863, 0.0273972602739726, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0136986301369863, 0.0547945205479452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726, 0.0273972602739726]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_1, tfidf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity\n",
    "\n",
    "- IoU = (Intersection of two lists) / (Union of two lists)\n",
    "- This can be used for two word lists where the IoU gives the similarity of all the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(words1, words2):\n",
    "    l1 = len(words1)\n",
    "    l2 = len(words2)\n",
    "    inter = list(set(words1) & set(words2))\n",
    "\n",
    "    print(l1,l2,inter, len(inter))\n",
    "\n",
    "    iou = len(inter) / (l1+l2-len(inter))\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 73 ['if', 'you', 'after', 'individual', 'find', 'rewards', 'any', 'savings', 'of', 'receipt', 'the', 'and', 'for', 'based', 'do', 'receipt,', 'have', 'points', 'way', 'earn', 'just', 'to', 'products', 'already', 'brands', 'or', 'with', 'we', 'is', 'will', 'coupons', 'shop', 'cost', '.', 'your', 'easiest', 'scan', 'on', 'not', 'get', 'fetch', 'need'] 42\n",
      "0.4117647058823529\n"
     ]
    }
   ],
   "source": [
    "print(jaccard_sim(words1, words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "- After converting words to vectors, we can compute the similarity between the vectors by getting the cosine of the angle between them.\n",
    "- The dot product of two vectors gives us the cosine of the angle between them.\n",
    "- The dot product is calculated as (sum of products of individual vector components) / (product of magnitudes of two vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "def square_rooted(x):\n",
    "    return round(sqrt(sum([a*a for a in x])),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(tfidf_1,tfidf_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidian Distance\n",
    "\n",
    "- It is nothng but the direct distance between the two points in space.\n",
    "- As we have word vectors, we can compute the distance between each vector component using distance formula.\n",
    "- euclidian distance = sq.root(sum over all components i (xi - yi)^2)\n",
    "- This gives the direct distance but we need a metric in 0 to 1.\n",
    "- This is achieved by introducing a threshold value.\n",
    "- The similarity = (threshold - distance) / threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x,y):\n",
    "    dist = round(sqrt(sum(pow(a-b,2) for a, b in zip(x, y))),3)\n",
    "    \n",
    "    thresh = 10\n",
    "    if dist > thresh:\n",
    "        return 0\n",
    "    else:\n",
    "        return (thresh - dist)/thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9883\n"
     ]
    }
   ],
   "source": [
    "print(euclidean_distance(tfidf_1,tfidf_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
